# 강화학습(Reinforcement Learning, RL) 개요

강화학습은 에이전트가 **환경(Environment)**과 상호작용하며 **보상(Reward)**을 최대화하는 행동(**Policy**)을 스스로 학습하는 기계학습 분야입니다.

- **에이전트(Agent)**: 의사결정 주체  
- **환경(Environment)**: 에이전트가 관찰하고, 행동을 취하며, 보상을 받는 대상  
- **상태(State, s)**: 에이전트가 환경을 인식한 정보  
- **행동(Action, a)**: 에이전트가 상태에서 취할 수 있는 선택지  
- **보상(Reward, r)**: 행동의 결과로 환경이 에이전트에 주는 즉시적 가치  
- **정책(Policy, π(a|s))**: 주어진 상태에서 행동을 선택할 확률 분포  

---

## 1. 마르코프 결정 과정(Markov Decision Process, MDP)

강화학습은 보통 MDP 프레임워크로 모델링합니다.

- 상태 집합 \(\mathcal{S}\), 행동 집합 \(\mathcal{A}\)  
- 전이확률 \(P(s' \mid s,a)\): 상태 \(s\)에서 행동 \(a\)를 취했을 때 다음 상태가 \(s'\)가 될 확률  
- 보상 함수 \(R(s,a,s')\): 전이 후 받는 보상  

강화학습의 목표는 **누적 보상(Return)**

\[
G_t = \sum_{k=0}^\infty \gamma^k R_{t+k+1},\quad 0 \le \gamma < 1
\]

를 최대화하는 정책 \(\pi\)를 찾는 것입니다.

---

## 2. 가치함수(Value Function)

가치함수는 장기 보상의 기대값을 평가하는 지표로, 두 가지 종류가 있습니다.

1. **상태 가치함수** \(V^\pi(s)\)  
   \[
   V^\pi(s) = \mathbb{E}_\pi\bigl[G_t \mid S_t = s\bigr].
   \]

2. **행동 가치함수** \(Q^\pi(s,a)\)  
   \[
   Q^\pi(s,a) = \mathbb{E}_\pi\bigl[G_t \mid S_t = s,\;A_t = a\bigr].
   \]

---

## 3. 벨만 기대 방정식(Bellman Expectation Equation)

가치함수는 다음 재귀식으로 표현됩니다.

- **상태 가치함수**  
  \[
  V^\pi(s)
    = \sum_{a}\pi(a\mid s)\sum_{s',r}P(s',r \mid s,a)\bigl[r + \gamma\,V^\pi(s')\bigr].
  \]

- **행동 가치함수**  
  \[
  Q^\pi(s,a)
    = \sum_{s',r}P(s',r \mid s,a)\Bigl[r + \gamma\sum_{a'}\pi(a'\mid s')\,Q^\pi(s',a')\Bigr].
  \]

---

## 4. 깊이 제한 재귀 탐색 (Depth-Limited Lookahead)

코드에서는 `max_step` (depth_limit)을 설정하여 탐색 깊이를 제한했습니다.

- **깊이 0일 때**  
  - \(V_0(s) = \sum_a \pi(a\mid s)\,R(s,a)\)  
  - \(Q_0(s,a) = R(s,a)\)

- **깊이 \(d>0\)일 때**  
  \[
  V_d(s)
    = \sum_a \pi(a\mid s)\Bigl[R(s,a) + \gamma\,V_{d-1}(s')\Bigr],
  \]
  \[
  Q_d(s,a)
    = R(s,a) + \gamma\,\sum_{a'}\pi(a'\mid s')\,Q_{d-1}(s',a').
  \]

각 재귀 호출마다 `depth+1`을 넘겨 다음 단계 보상을 계산합니다.

---

## 5. 균일 랜덤 정책 (Uniform Random Policy)

\[
\pi(a\mid s) = 0.25
\]

- 예제에서는 균일 확률 정책을 사용하여 **정책 평가(Policy Evaluation)** 과정을 데모합니다.  
- 실제 학습 알고리즘(Q-Learning, Policy Gradient 등)에서는 이 정책을 점진적으로 개선합니다.

---

## 6. 정책 추출 (Policy Extraction)

평가된 Q-테이블에서 **탐욕적(greedy)** 으로 정책을 뽑습니다.

\[
\pi^*(s) = \arg\max_a Q(s,a).
\]

코드의 `extract_policy` 함수는 상태별 Q 값이 최대인 행동 인덱스를 반환합니다.

---

## 7. 계산 복잡도 및 벤치마킹

- **탐색 깊이 \(d\)** 에 대해, 한 상태에서 수행하는 재귀 호출 수는 약 \(\lvert\mathcal{A}\rvert^d\)  
- 전체 비용: \(\mathcal{O}\bigl(\lvert\mathcal{S}\rvert \times \lvert\mathcal{A}\rvert^d\bigr)\)  
- `benchmark_v` 함수는 깊이별 `compute_v` 실행 시간을 측정하고 시각화하여, 지수적 증가를 확인할 수 있습니다.

---

## 8. 실제 학습 알고리즘으로의 확장

1. **정책 개선(Policy Improvement)**: 평가된 \(Q^\pi\)로부터 새로운 정책 \(\pi'\) 추출  
2. **정책 반복(Policy Iteration)**: \(\pi\) 평가 → \(\pi\) 개선 과정을 반복  
3. **값 반복(Value Iteration)**: 벨만 최적 방정식으로 \(V^*(s)\) 계산  
4. **Q-Learning**, **SARSA**: 온라인 업데이트  
5. **정책 그래디언트**, **Actor-Critic**: 파라미터화된 정책 학습  

---

**요약**

1. MDP 모델 정의  
2. 벨만 방정식으로 \(V\)와 \(Q\) 재귀적 정의  
3. 깊이 제한 탐색 구현  
4. 균일 랜덤 정책으로 평가  
5. 탐욕적 정책 추출  
6. 계산 복잡도 이해 및 벤치마킹  

