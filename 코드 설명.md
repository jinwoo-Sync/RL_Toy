## MDP 형식화: `Environment` & `Agent`

| 코드 부분                                    | 이론적 개념                                                                                               |
|---------------------------------------------|----------------------------------------------------------------------------------------------------------|
| ```python<br>class Environment:<br>```      | **MDP 정의**<br>- 상태 공간 \(\mathcal{S}\): (0,0) 부터 (2,3) 까지의 격자 좌표<br>- 행동 공간 \(\mathcal{A}\): 위/우/아래/좌 |
| `self.rewards` 배열                         | **보상 함수** \(R(s,a,s')\) 구현: 이동한 칸의 숫자 보상 (도로, 절벽, 싱크, 목표)                            |
| `self.labels` 리스트                        | 터미널 상태 구분 (‘goal’ 체크용)                                                                           |
| `move(self, agent, action_idx)`             | **전이 확률 \(P\)**과 보상 결합: 결정적 전이 (즉, \(s,a\to s'\)이 100%)                                    |
| ```python<br>class Agent:<br>    actions = np.array([...])<br>    prob = np.full(4,0.25)<br>``` | **정책 \(\pi(a\mid s)\)**: 균일 랜덤 정책 (학습 없이 확률 0.25로 행동 선택)<br>에이전트의 위치(`agent.pos`) = 현재 상태 \(s\) |

---
| 코드 모듈                 | 이론            | 핵심 역할                        |
| --------------------- | ------------- | ---------------------------- |
| `Environment`/`Agent` | MDP 구성요소      | 상태·행동·보상 정의 및 전이 함수          |
| `compute_q`           | 벨만 기대 방정식 (Q) | 깊이 제한 재귀로 $Q^\pi(s,a)$ 계산    |
| `compute_v`           | 벨만 기대 방정식 (V) | 깊이 제한 재귀로 $V^\pi(s)$ 계산      |
| `extract_policy`      | 탐욕적 정책 추출     | $Q$ 에서 $\arg\max$ 로 최적 행동 선택 |
| `benchmark_v`         | 복잡도/성능 분석     | 깊이에 따른 연산 시간 측정 및 시각화        |
| `show_policy`         | 정책 시각화        | 2D 화살표 출력                    |




## 2️⃣ 행동 가치 함수 \(Q^\pi(s,a)\): `compute_q`

```python
def compute_q(env, agent, depth_limit, gamma=0.9):
    Q = np.zeros((*env.rewards.shape, len(agent.actions)))
    def q_val(pos, action, depth):
        agent.reset(pos)
        if env.labels[pos[0]][pos[1]] == 'goal':
            return env.GOAL
        _, r, done = env.move(agent, action)
        if depth == depth_limit or done:
            return r
        future = sum(agent.prob[a] * q_val(agent.pos.copy(), a, depth+1)
                     for a in range(len(agent.actions)))
        return r + gamma * future

    for i in range(env.rewards.shape[0]):
        for j in range(env.rewards.shape[1]):
            for a in range(len(agent.actions)):
                Q[i,j,a] = q_val((i,j), a, 0)
    return Q


### Bellman 기대 방정식

#### 행동 가치 함수 \(Q_d^\pi(s,a)\)

\[  
Q_d^\pi(s,a) = R(s,a) + \gamma \sum_{a'} \pi(a' \mid s')\,Q_{d-1}^\pi(s',a')  
\]

- **의미**  
  - \(R(s,a)\): 지금 한 번 행동 \(a\)를 취했을 때 즉시 얻는 보상  
  - \(\gamma \sum_{a'} \pi(a' \mid s')\,Q_{d-1}^\pi(s',a')\): 다음 상태 \(s'\)에서의 장기 보상의 기대값을 할인율 \(\gamma\)만큼 곱해 더한 값  
- **깊이 제한**  
  - \(d = 0\)일 때:  
    \[
      Q_0^\pi(s,a) = R(s,a)
    \]  
    → 한 단계 보상만 고려  
  - \(d > 0\)일 때:  
    재귀적으로 \(Q_{d-1}^\pi\)를 사용해 미래를 내다봄

#### 상태 가치 함수 \(V_d^\pi(s)\)

\[  
V_d^\pi(s) = \sum_{a} \pi(a \mid s)\,\bigl[R(s,a) + \gamma\,V_{d-1}^\pi(s')\bigr]  
\]

- **의미**  
  - 상태 \(s\)에서 정책 \(\pi\)에 따라 행동 \(a\)를 선택할 확률 \(\pi(a\mid s)\)로 가중합  
  - 즉시 보상 \(R(s,a)\)와 다음 상태 \(s'\)의 가치 \(V_{d-1}^\pi(s')\)를 합산  
- **깊이 제한**  
  - \(d = 0\)일 때:  
    \[
      V_0^\pi(s) = \sum_a \pi(a\mid s)\,R(s,a)
    \]  
    → 한 단계 기대 보상만 계산

---

### 탐욕적 정책 (Greedy Policy)

\[  
\pi^*(s) = \arg\max_{a} Q^\pi(s,a)  
\]

- 각 상태 \(s\)에서 가장 큰 \(Q\) 값을 주는 행동 \(a\)를 선택  
- 출력: 2D 배열 형태로, 격자마다 0~3 사이의 행동 인덱스를 가짐

---

### 계산 복잡도 및 벤치마킹

- **복잡도**:  
  \[
    O\bigl(\lvert S\rvert \times \lvert A\rvert^{d}\bigr)
  \]  
  - \(\lvert S\rvert\): 상태 수  
  - \(\lvert A\rvert\): 행동 수  
  - \(d\): 깊이 제한  
- **벤치마킹**:  
  - `benchmark_v(env, agent, depths)` 함수로 \(d = 0,1,\dots,7\)에 대해 `compute_v` 실행 시간을 측정  
  - `matplotlib` 으로 그래프를 그려 지수적 증가 추세를 확인
