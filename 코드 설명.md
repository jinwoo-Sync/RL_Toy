## MDP 형식화: `Environment` & `Agent`

| 코드 부분                                    | 이론적 개념                                                                                               |
|---------------------------------------------|----------------------------------------------------------------------------------------------------------|
| ```python<br>class Environment:<br>```      | **MDP 정의**<br>- 상태 공간 \(\mathcal{S}\): (0,0) 부터 (2,3) 까지의 격자 좌표<br>- 행동 공간 \(\mathcal{A}\): 위/우/아래/좌 |
| `self.rewards` 배열                         | **보상 함수** \(R(s,a,s')\) 구현: 이동한 칸의 숫자 보상 (도로, 절벽, 싱크, 목표)                            |
| `self.labels` 리스트                        | 터미널 상태 구분 (‘goal’ 체크용)                                                                           |
| `move(self, agent, action_idx)`             | **전이 확률 \(P\)**과 보상 결합: 결정적 전이 (즉, \(s,a\to s'\)이 100%)                                    |
| ```python<br>class Agent:<br>    actions = np.array([...])<br>    prob = np.full(4,0.25)<br>``` | **정책 \(\pi(a\mid s)\)**: 균일 랜덤 정책 (학습 없이 확률 0.25로 행동 선택)<br>에이전트의 위치(`agent.pos`) = 현재 상태 \(s\) |

---
| 코드 모듈                 | 이론            | 핵심 역할                        |
| --------------------- | ------------- | ---------------------------- |
| `Environment`/`Agent` | MDP 구성요소      | 상태·행동·보상 정의 및 전이 함수          |
| `compute_q`           | 벨만 기대 방정식 (Q) | 깊이 제한 재귀로 $Q^\pi(s,a)$ 계산    |
| `compute_v`           | 벨만 기대 방정식 (V) | 깊이 제한 재귀로 $V^\pi(s)$ 계산      |
| `extract_policy`      | 탐욕적 정책 추출     | $Q$ 에서 $\arg\max$ 로 최적 행동 선택 |
| `benchmark_v`         | 복잡도/성능 분석     | 깊이에 따른 연산 시간 측정 및 시각화        |
| `show_policy`         | 정책 시각화        | 2D 화살표 출력                    |



## 벨만 기대 방정식 (Bellman Expectation Equation)

### 행동 가치 함수 $Q^\pi_d(s, a)$

$Q^\pi_d(s, a) = \mathbb{E}_{\pi}[R(s, a) + \gamma \sum_{a'} \pi(a' | s') Q^\pi_{d-1}(s', a')]$

**의미**

1.  $R(s, a)$: 즉시 얻는 보상
2.  $\gamma \sum_{a'} \pi(a' | s') Q^\pi_{d-1}(s', a')$: 다음 상태 $s'$에서의 기대 장기 보상을 할인율 $\gamma$만큼 곱해 현재 가치로 나타냄

**깊이 제한**

* $d = 0$일 때:
    $Q^\pi_0(s, a) = R(s, a)$
    $\implies$ 한 단계 보상만 고려
* $d > 0$일 때: 재귀적으로 $Q^\pi_{d-1}$를 호출

### 상태 가치 함수 $V^\pi_d(s)$

$V^\pi_d(s) = \mathbb{E}_{\pi}[\sum_{a} \pi(a | s) [R(s, a) + \gamma V^\pi_{d-1}(s')]]$

**의미**

* 상태 $s$에서 행동 $a$를 선택할 확률 $\pi(a | s)$로 가중합
* 즉시 보상 $R(s, a)$와 다음 상태 $s'$의 가치 $V^\pi_{d-1}(s')$를 합산

**깊이 제한**

* $d = 0$일 때:
    $V^\pi_0(s) = \sum_{a} \pi(a | s) R(s, a)$
    $\implies$ 한 단계 기대 보상 계산

## 탐욕적 정책 (Greedy Policy)

$\pi^*(s) = \text{argmax}_a Q^\pi(s, a)$

* 각 상태 $s$에서 가장 큰 $Q$ 값을 주는 행동 $a$를 선택
* 결과: 2D 배열 형태로, 각자마다 0~3 사이의 행동 인덱스를 가짐

## 계산 복잡도 및 벤치마킹

### 복잡도

$O(|S| \times |A|^d)$

* $|S|$: 상태 수
* $|A|$: 행동 수
* $d$: 깊이 제한

### 벤치마킹

* `benchmark_v(env, agent, depths)` 함수로 $d = 0, 1, ..., 7$에 대해 `compute_v` 실행 시간을 측정
* `matplotlib`으로 실행 시간 그래프를 그려 지수적 증가 추세를 확인
